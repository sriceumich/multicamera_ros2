#!/usr/bin/env python3
import asyncio
import json
import base64
import threading
import time
import os
import os.path as osp
from importlib.resources import files
import sqlite3
from datetime import datetime
from typing import Dict, Any, List, Optional
from sensor_msgs.msg import PointCloud2
import sensor_msgs_py.point_cloud2 as pc2
from sensor_msgs.msg import Imu
from std_msgs.msg import Int32
import transforms3d.euler as euler
from fastapi.staticfiles import StaticFiles
import random

import cv2
import rclpy
from rclpy.node import Node
from rclpy.qos import QoSProfile, ReliabilityPolicy, HistoryPolicy, QoSPresetProfiles
from sensor_msgs.msg import Image
from std_msgs.msg import String
from vision_msgs.msg import Detection2DArray
from cv_bridge import CvBridge

from fastapi import FastAPI, WebSocket, Query
from fastapi.responses import HTMLResponse, FileResponse, JSONResponse
import uvicorn

# ---------------- Config ----------------
RECORD_ROOT = os.environ.get("RECORD_ROOT", "/data/recordings")
os.makedirs(RECORD_ROOT, exist_ok=True)
DB_PATH = osp.join(RECORD_ROOT, "recordings.db")

app = FastAPI()
bridge = CvBridge()
static_dir = osp.join(osp.dirname(__file__), "static")
if osp.exists(static_dir):
    app.mount("/static", StaticFiles(directory=static_dir), name="static")
else:
    print(f"âš ï¸ static dir not found: {static_dir}")

# ---------------- Shared state ----------------
latest_raw_frames: Dict[str, Optional[str]] = {}  # {cam_id: b64_jpg}
latest_ann_frames: Dict[str, Optional[str]] = {}  # {cam_id: b64_jpg}
latest_detections: Dict[str, List[Dict[str, Any]]] = {}  # {cam_id: [{class,score,bbox}]}
latest_alerts: List[Dict[str, Any]] = []
known_cameras = set()

latest_embeddings: Dict[str, Any] = {}     # {cam_id: parsed_json_or_raw}
latest_df_analysis: Dict[str, Any] = {}    # {cam_id: parsed_json_or_raw}
latest_df_ann_frames: Dict[str, Optional[str]] = {}  # deepface annotated images b64

# --- Motion state (per camera) ---
_prev_small_raw = {}      # cam -> small grayscale frame (numpy)
_prev_small_ann = {}      # cam -> small grayscale frame (numpy)
_motion_until = {}        # cam -> unix_ts until which we consider "in motion"
MOTION_HOLD_SECS = 1.5    # keep highlight this long after last motion
MOTION_THRESH = 8.0       # mean abs-diff threshold on downscaled frames

latest_lidar_points = []
latest_lidar_imu = None       # {roll, pitch, yaw}
latest_lidar_loss = 0

recording_lock = threading.Lock()
recording_enabled = False
recording_session: Optional[str] = None

def _motion_update(cam_id: str, img_bgr, stream_type: str):
    """Update motion state using small-frame absdiff; set _motion_until[cam] if motion."""
    try:
        small = cv2.resize(img_bgr, (96, 54))                # tiny to keep CPU light
        gray = cv2.cvtColor(small, cv2.COLOR_BGR2GRAY)
        key = (_prev_small_ann if stream_type == "annotated" else _prev_small_raw)
        prev = key.get(cam_id)
        key[cam_id] = gray
        if prev is None:
            return
        diff = cv2.absdiff(prev, gray)
        mean = float(diff.mean())
        if mean >= MOTION_THRESH:
            _motion_until[cam_id] = time.time() + MOTION_HOLD_SECS
    except Exception:
        pass

# ---------------- SQLite helpers ----------------
def db_init():
    conn = sqlite3.connect(DB_PATH)
    c = conn.cursor()
    c.execute("""
    CREATE TABLE IF NOT EXISTS frames(
        session TEXT,
        camera  TEXT,
        ts      REAL,
        kind    TEXT,   -- 'raw' | 'annotated'
        path    TEXT,
        PRIMARY KEY(session, camera, ts, kind)
    ) WITHOUT ROWID;
    """)
    c.execute("""
    CREATE TABLE IF NOT EXISTS detections(
        session TEXT,
        camera  TEXT,
        ts      REAL,
        class   TEXT,
        score   REAL,
        bbox    TEXT
    );
    """)
    c.execute("CREATE INDEX IF NOT EXISTS idx_frames_cam ON frames(session, camera, kind, ts);")
    c.execute("CREATE INDEX IF NOT EXISTS idx_det_cam ON detections(session, camera, class, ts);")
    conn.commit()
    conn.close()

def db_execute(sql: str, args: tuple = ()):
    conn = sqlite3.connect(DB_PATH)
    c = conn.cursor()
    c.execute(sql, args)
    conn.commit()
    conn.close()

def db_query(sql: str, args: tuple = ()) -> List[tuple]:
    conn = sqlite3.connect(DB_PATH)
    c = conn.cursor()
    c.execute(sql, args)
    rows = c.fetchall()
    conn.close()
    return rows

db_init()

def ts_filename(ts: float) -> str:
    dt = datetime.fromtimestamp(ts)
    return dt.strftime("%Y%m%d_%H%M%S_") + f"{int((ts - int(ts)) * 1000):03d}.jpg"

def ensure_dir(path: str):
    os.makedirs(path, exist_ok=True)

# ---------------- ROS Node ----------------
class WebStreamNode(Node):
    def __init__(self):
        super().__init__("wildlife_webstream")
        self.bridge = CvBridge()
        self.subs = []
        self._known_topics = set()

        # Initial subscribe + periodic refresh
        self._setup_initial_subs()
        self.create_timer(5.0, self._refresh_topics)

        # Alerts (reliable ok)
        qos = QoSProfile(
            depth=10,
            reliability=ReliabilityPolicy.BEST_EFFORT,
            history=HistoryPolicy.KEEP_LAST
        )
        self.alert_sub = self.create_subscription(String, "/wildlife/alert", self.alert_cb, 10)
        self.lidar_sub = self.create_subscription(PointCloud2, "/lidar_points", self.lidar_cb, qos)
        self.imu_sub   = self.create_subscription(Imu, "/lidar_imu", self.imu_cb, 10)
        self.loss_sub  = self.create_subscription(Int32, "/lidar_packets_loss", self.loss_cb, 10)

    def imu_cb(self, msg: Imu):
        global latest_lidar_imu
        q = msg.orientation
        quat = [q.x, q.y, q.z, q.w]
        r, p, y = euler.quat2euler(quat)
        latest_lidar_imu = {"roll": r, "pitch": p, "yaw": y}

    def loss_cb(self, msg: Int32):
        global latest_lidar_loss
        latest_lidar_loss = msg.data

    def lidar_cb(self, msg: PointCloud2):
        global latest_lidar_points
        t0 = time.time()
        all_points = []

        for p in pc2.read_points(
            msg, field_names=("x", "y", "z", "intensity"), skip_nans=True
        ):
            all_points.append([float(p[0]), float(p[1]), float(p[2]), float(p[3])])
        print(f"lidar_cb decode took {(time.time()-t0)*1000:.1f} ms for {len(all_points)} pts")

        total = len(all_points)
        if total == 0:
            latest_lidar_points = []
            return

        # keep at most 20k points, or 10% if less
        target = min(total, total) # int(total * 0.1))
        print(f"lidar_cb min took {(time.time()-t0)*1000:.1f} ms for {len(all_points)} pts")

        if total <= target:
            sampled = all_points
        else:
            # uniform random subsample
            sampled = random.sample(all_points, target)

        latest_lidar_points = sampled
        print(f"lidar_cb random took {(time.time()-t0)*1000:.1f} ms for {len(all_points)} pts")
        print(f"âš¡ lidar_cb got {total} pts â†’ sending {len(sampled)}")

    # ---- Topic discovery & subscribe ----
    def _discover_topics(self):
        topics = {
            "raw": [],
            "annotated": [],
            "detections": [],
            "embeddings": [],
            "df_analysis": [],
            "df_annotated": [],
        }
        for name, types in self.get_topic_names_and_types():
            if name.startswith("/cameras/") and name.endswith("/image_raw") and "sensor_msgs/msg/Image" in types:
                topics["raw"].append(name)
            if name.startswith("/wildlife/annotated/") and "sensor_msgs/msg/Image" in types:
                topics["annotated"].append(name)
            if name.startswith("/wildlife/detections/") and "vision_msgs/msg/Detection2DArray" in types:
                topics["detections"].append(name)

            #  DINOV3 embeddings (String JSON)
            if name.startswith("/wildlife/embeddings/") and "std_msgs/msg/String" in types:
                topics["embeddings"].append(name)

            #  DeepFace analysis (String JSON)
            if name.startswith("/deepface/analysis/") and "std_msgs/msg/String" in types:
                topics["df_analysis"].append(name)

            #  DeepFace annotated images
            if name.startswith("/deepface/annotated/") and "sensor_msgs/msg/Image" in types:
                topics["df_annotated"].append(name)
        return topics

    def _setup_initial_subs(self):
        topics = self._discover_topics()
        self._subscribe_sets(topics)
        self.get_logger().info(f"Initial discovery: {topics}")

    def _refresh_topics(self):
        topics = self._discover_topics()
        self._subscribe_sets(topics)

    @staticmethod
    def _try_json_loads(s: str) -> Any:
        try:
            return json.loads(s)
        except Exception:
            return s  # keep raw if not valid JSON

    # NEW: DINOV3 embeddings
    def emb_cb(self, msg: String, cam_id: str):
        try:
            payload = self._try_json_loads(msg.data or "")
            latest_embeddings[cam_id] = payload
        except Exception as e:
            self.get_logger().error(f"Embeddings parse error (cam={cam_id}): {e}")

    # NEW: DeepFace analysis
    def df_analysis_cb(self, msg: String, cam_id: str):
        try:
            payload = self._try_json_loads(msg.data or "")
            latest_df_analysis[cam_id] = payload
        except Exception as e:
            self.get_logger().error(f"DF analysis parse error (cam={cam_id}): {e}")

    # NEW: DeepFace annotated images
    def _handle_df_ann_frame(self, msg: Image, cam_id: str):
        try:
            img = self.bridge.imgmsg_to_cv2(msg, desired_encoding="bgr8")
            self.get_logger().info(f"DF frame {cam_id}: {img.shape}")
            _motion_update(cam_id, img, "annotated")  # reuse motion highlighting
            ok, buf = cv2.imencode(".jpg", img)
            if not ok:
                return
            b64 = base64.b64encode(buf).decode("utf-8")
            latest_df_ann_frames[cam_id] = b64
        except Exception as e:
            self.get_logger().error(f"DF Ann frame error ({cam_id}): {e}")


    def _subscribe_sets(self, topics):
        # RAW images (BEST_EFFORT/SENSOR_DATA)
        for t in topics["raw"]:
            if t in self._known_topics:
                continue
            cam_id = self._cam_id_from_topic(t)
            self.get_logger().info(f"Subscribing RAW: {t} (id={cam_id})")
            self.subs.append(
                self.create_subscription(
                    Image, t,
                    lambda msg, cid=cam_id: self._handle_frame(msg, "raw", cid),
                    qos_profile=QoSPresetProfiles.SENSOR_DATA.value
                )
            )
            self._known_topics.add(t)
            known_cameras.add(cam_id)
            latest_raw_frames.setdefault(cam_id, None)

        # Annotated images (BEST_EFFORT/SENSOR_DATA)
        for t in topics["annotated"]:
            if t in self._known_topics:
                continue
            cam_id = self._cam_id_from_topic(t)
            self.get_logger().info(f"Subscribing ANN: {t} (id={cam_id})")
            self.subs.append(
                self.create_subscription(
                    Image, t,
                    lambda msg, cid=cam_id: self._handle_frame(msg, "annotated", cid),
                    qos_profile=QoSPresetProfiles.SENSOR_DATA.value
                )
            )
            self._known_topics.add(t)
            known_cameras.add(cam_id)
            latest_ann_frames.setdefault(cam_id, None)

        # Detections (reliable ok)
        for t in topics["detections"]:
            if t in self._known_topics:
                continue
            cam_id = self._cam_id_from_topic(t)
            self.get_logger().info(f"Subscribing DET: {t} (id={cam_id})")
            self.subs.append(
                self.create_subscription(
                    Detection2DArray, t,
                    lambda msg, cid=cam_id: self.det_cb(msg, cid), 10
                )
            )
            self._known_topics.add(t)
            known_cameras.add(cam_id)
            latest_detections.setdefault(cam_id, [])


        # NEW: DINOV3 embeddings (reliable ok)
        for t in topics["embeddings"]:
            if t in self._known_topics:
                continue
            cam_id = self._cam_id_from_topic(t)
            self.get_logger().info(f"Subscribing EMBEDDINGS: {t} (id={cam_id})")
            self.subs.append(
                self.create_subscription(
                    String, t, lambda msg, cid=cam_id: self.emb_cb(msg, cid), 10
                )
            )
            self._known_topics.add(t)
            known_cameras.add(cam_id)
            latest_embeddings.setdefault(cam_id, None)

        # NEW: DeepFace analysis (reliable ok)
        for t in topics["df_analysis"]:
            if t in self._known_topics:
                continue
            cam_id = self._cam_id_from_topic(t)
            self.get_logger().info(f"Subscribing DF_ANALYSIS: {t} (id={cam_id})")
            self.subs.append(
                self.create_subscription(
                    String, t, lambda msg, cid=cam_id: self.df_analysis_cb(msg, cid), 10
                )
            )
            self._known_topics.add(t)
            known_cameras.add(cam_id)
            latest_df_analysis.setdefault(cam_id, None)

        # NEW: DeepFace annotated images (BEST_EFFORT/SENSOR_DATA)
        for t in topics["df_annotated"]:
            if t in self._known_topics:
                continue
            cam_id = self._cam_id_from_topic(t)
            self.get_logger().info(f"Subscribing DF_ANN: {t} (id={cam_id})")
            self.subs.append(
                self.create_subscription(
                    Image, t,
                    lambda msg, cid=cam_id: self._handle_df_ann_frame(msg, cid),
                    qos_profile=QoSPresetProfiles.SENSOR_DATA.value
                )
            )
            self._known_topics.add(t)
            known_cameras.add(cam_id)
            latest_df_ann_frames.setdefault(cam_id, None)
            
    # ---- cam_id parsing ----
    def _cam_id_from_topic(self, topic: str) -> str:
        if topic.startswith("/cameras/") and topic.endswith("/image_raw"):
            return topic.split("/")[-2]
        for prefix in (
            "/wildlife/annotated/",
            "/wildlife/detections/",
            "/wildlife/embeddings/",
            "/deepface/analysis/",
            "/deepface/annotated/",
        ):
            if topic.startswith(prefix):
                return topic.split("/")[-1]
        return topic.split("/")[-1]
        
    # ---- Callbacks ----
    def _handle_frame(self, msg: Image, stream_type: str, cam_id: str):
        try:
            img = self.bridge.imgmsg_to_cv2(msg, desired_encoding="bgr8")
            # --- motion tracking ---
            _motion_update(cam_id, img, stream_type)

            ok, buf = cv2.imencode(".jpg", img)
            if not ok:
                return
            b64 = base64.b64encode(buf).decode("utf-8")
            if stream_type == "raw":
                latest_raw_frames[cam_id] = b64
            else:
                latest_ann_frames[cam_id] = b64

            # Recording
            with recording_lock:
                if recording_enabled and recording_session:
                    ts = msg.header.stamp.sec + msg.header.stamp.nanosec / 1e9
                    session_dir = osp.join(RECORD_ROOT, recording_session, cam_id, stream_type)
                    ensure_dir(session_dir)
                    fn = ts_filename(ts)
                    fpath = osp.join(session_dir, fn)
                    with open(fpath, "wb") as f:
                        f.write(buf.tobytes())
                    db_execute(
                        "INSERT OR IGNORE INTO frames(session,camera,ts,kind,path) VALUES (?,?,?,?,?)",
                        (recording_session, cam_id, ts, stream_type, fpath)
                    )
        except Exception as e:
            self.get_logger().error(f"Frame error ({cam_id}/{stream_type}): {e}")

    def det_cb(self, msg: Detection2DArray, cam_id: str):
        dets = []
        ts = msg.header.stamp.sec + msg.header.stamp.nanosec / 1e9 if msg and msg.header and msg.header.stamp else time.time()
        for d in msg.detections:
            for hyp in d.results:
                try:
                    center = d.bbox.center
                    cx = getattr(center, "x", None)
                    cy = getattr(center, "y", None)
                    if cx is None or cy is None:
                        pos = getattr(center, "position", None)
                        if pos is not None:
                            cx, cy = pos.x, pos.y
                    entry = {
                        "class": hyp.hypothesis.class_id,
                        "score": float(hyp.hypothesis.score),
                        "bbox": [float(cx or 0.0), float(cy or 0.0),
                                 float(d.bbox.size_x), float(d.bbox.size_y)]
                    }
                    dets.append(entry)

                    # Recording index (annotation search)
                    with recording_lock:
                        if recording_enabled and recording_session:
                            db_execute(
                                "INSERT INTO detections(session,camera,ts,class,score,bbox) VALUES (?,?,?,?,?,?)",
                                (recording_session, cam_id, ts, str(entry["class"]), float(entry["score"]), json.dumps(entry["bbox"]))
                            )
                except Exception as e:
                    self.get_logger().error(f"Detection parse error (cam={cam_id}): {e}")
        latest_detections[cam_id] = dets

    def alert_cb(self, msg: String):
        try:
            latest_alerts.append(json.loads(msg.data))
            if len(latest_alerts) > 200:
                latest_alerts.pop(0)
        except Exception:
            pass

# ---------------- HTTP UI (Tabbed Dashboard w/ Record & Playback & Stats) ----------------
@app.get("/")
async def index():
    return HTMLResponse("""
<!DOCTYPE html>
<html>
<head>
  <meta charset="utf-8"/>
  <title>Wildlife Dashboard</title>
  <style>
    body { font-family: system-ui, -apple-system, Segoe UI, Roboto, sans-serif; background:#0f1115; color:#e5e7eb; margin:0; }
    header { padding:14px 18px; background:#151923; border-bottom:1px solid #232836; display:flex; align-items:center; gap:12px; }
    h1 { font-size:18px; margin:0; }
    .tabs { display:flex; gap:8px; margin-left:auto; flex-wrap:wrap;}
    .tab { padding:8px 12px; background:#1d2330; border:1px solid #2a3040; border-radius:8px; cursor:pointer; }
    .tab.active { background:#334155; }
    .section { display:none; padding:16px; }
    .section.active { display:block; }
    .grid { display:grid; grid-template-columns:repeat(auto-fill,minmax(360px,1fr)); gap:12px; }
    .tile { background:#161b26; border:1px solid #232836; border-radius:10px; overflow:hidden; }
    .tile header { display:flex; align-items:center; justify-content:space-between; padding:8px 12px; background:#0f1420; border-bottom:1px solid #232836; }
    .tile h3 { margin:0; font-size:14px; color:#cbd5e1; }
    .imgwrap { background:#0b0e14; display:flex; align-items:center; justify-content:center; min-height:200px; }
    img { display:block; width:100%; height:auto; cursor:pointer; }
    .meta { font-size:12px; color:#a1a7b5; padding:8px 12px; }
    .alertbox { padding:12px; background:#2a0f10; border:1px solid #5b1d1f; border-radius:10px; margin:12px; }
    .pill { font-size:11px; padding:2px 8px; border-radius:999px; border:1px solid #374151; color:#cbd5e1; }
    .controls { display:flex; gap:8px; align-items:center; flex-wrap:wrap; margin:8px 0; }
    input[type="text"], select { background:#0f1420; color:#e5e7eb; border:1px solid #2a3040; border-radius:8px; padding:6px 10px; }
    button { background:#1e293b; color:#e5e7eb; border:1px solid #2a3040; border-radius:8px; padding:6px 12px; cursor:pointer; }
    button.primary { background:#334155; }
    .row { display:flex; gap:12px; flex-wrap:wrap; }
    .col { flex:1 1 360px; }
    .timeline { width:100%; }
    svg { width:100%; height:320px; background:#0b0f17; border:1px solid #232836; border-radius:10px; }

    .tile {
      position: relative; /* for glow ball positioning */
      transition: transform .15s ease, box-shadow .15s ease, border-color .15s ease;
    }

    /* Motion highlight (animated gradient border) */
    .tile.motion {
      position: relative;
      border-color: #7dd3fc;
      box-shadow: 0 0 0 2px rgba(125,211,252,.35), 0 0 22px rgba(56,189,248,.25);
    }
    .tile.motion .orbit-wrap {
      position: absolute;
      inset: -8;            /* expand slightly outside tile */
      border-radius: 12px;    /* keep for shape; no rotation */
    }

    .tile.motion::before {
      content: "";
      position: absolute; inset: -2px;
      border-radius: 12px;
      padding: 2px; /* border thickness */
      background: conic-gradient(from 0deg, #22d3ee, #3b82f6, #22d3ee);
      -webkit-mask: linear-gradient(#000 0 0) content-box, linear-gradient(#000 0 0);
      -webkit-mask-composite: xor; mask-composite: exclude;
      animation: none;
    }
    /* Optional subtle pulse instead of rotation (purely visual) */
    @keyframes pulseBorder { 0%,100%{opacity:.9} 50%{opacity:.65} }
    /* If you want a faint breathing effect, uncomment next line: */
    /* .tile.motion::before { animation: pulseBorder 1.8s ease-in-out infinite; } */

    /* The glow ball travels the full rectangle outline */
    .tile.motion .glow-ball {
      position: absolute;
      width: 10px; height: 10px; border-radius: 50%;
      background: #fff;
      box-shadow: 0 0 12px 6px rgba(96,165,250,.55);

      /* Motion path (percentages scale with the box) */
      offset-path: path("M 0% 0% H 100% V 100% H 0% Z");
      -webkit-offset-path: path("M 0% 0% H 100% V 100% H 0% Z");

      /* Start distance explicitly at 0% */
      offset-distance: 0%;
      -webkit-offset-distance: 0%;

      /* Keep the ball orientation fixed (no rotation) */
      offset-rotate: 0deg;
      -webkit-offset-rotate: 0deg;

      /* Animate the distance along the path */
      animation: moveRect 2.2s linear infinite;
    }
    /* Animate both standard and webkit distances */
    @keyframes moveRect {
      from { offset-distance: 0%; }
      to   { offset-distance: 100%; }
    }
    @-webkit-keyframes moveRect {
      from { -webkit-offset-distance: 0%; }
      to   { -webkit-offset-distance: 100%; }
    }
    @keyframes orbit {
      0%   { top: -5px; left: 10%; }
      25%  { top: 10%; right: -5px; left: auto; }
      50%  { bottom: -5px; right: 10%; top: auto; left: auto; }
      75%  { bottom: 10%; left: -5px; top: auto; right: auto; }
      100% { top: -5px; left: 10%; bottom: auto; right: auto; }
    }

    /* Fullscreen helpers (works with the Fullscreen API) */
    .tile:fullscreen, .tile:-webkit-full-screen {
      width: 100vw; height: 100vh; border-radius: 0;
    }
    .tile:fullscreen .imgwrap, .tile:-webkit-full-screen .imgwrap {
      min-height: calc(100vh - 48px);
    }
    .tile .fs-hint {
      position:absolute; top:8px; right:10px; font-size:11px; opacity:.7;
    }

    #lidar {
      position: relative;
      width: 100vw;
      height: calc(100vh - 48px); /* minus header height */
      margin: 0;
      padding: 0;
    }

    #lidarCanvas {
      width: 100%;
      height: 100%;
      display: block;     /* remove default inline spacing */
    }

  </style>
</head>
<body>
  <header>
    <h1>Wildlife Monitoring Dashboard</h1>
    <div class="tabs">
      <div class="tab active" data-target="raw">Raw</div>
      <div class="tab" data-target="annotated">Annotated</div>
      <div class="tab" data-target="detections">Detections</div>
      <div class="tab" data-target="embeddings">Embeddings</div>        <!-- NEW -->
      <div class="tab" data-target="dfanalysis">DeepFace Analysis</div>  <!-- NEW -->
      <div class="tab" data-target="dfannotated">DeepFace Annotated</div><!-- NEW -->
      <div class="tab" data-target="alerts">Alerts</div>
      <div class="tab" data-target="record">Record</div>
      <div class="tab" data-target="playback">Playback</div>
      <div class="tab" data-target="stats">Stats</div>
      <div class="tab" data-target="lidar">LiDAR</div>
    </div>
  </header>

  <section id="raw" class="section active"><div id="rawGrid" class="grid"></div></section>
  <section id="annotated" class="section"><div id="annGrid" class="grid"></div></section>
  <section id="detections" class="section"><div id="detWrap" style="padding:8px 12px;"></div></section>
  <section id="alerts" class="section"><div id="alertWrap" style="padding:8px 12px;"></div></section>
  <section id="embeddings" class="section"><div id="embGrid" class="grid"></div></section>      <!-- NEW -->
  <section id="dfanalysis" class="section"><div id="dfaGrid" class="grid"></div></section>       <!-- NEW -->
  <section id="dfannotated" class="section"><div id="dfaAnnGrid" class="grid"></div></section>   <!-- NEW -->
  <section id="lidar" class="section" style="position:relative;">
    <canvas id="lidarCanvas"></canvas>
    <div id="lidarOverlay" style="position:absolute;top:10px;left:10px;
        background:#1d2330;padding:6px 12px;border-radius:8px;
        font-size:12px;color:#cbd5e1;">
      Loss: 0
    </div>
  </section>

  <section id="record" class="section">
    <div class="controls">
      <input id="sessionName" type="text" placeholder="Session name (optional)"/>
      <button class="primary" onclick="startRecord()">Start Recording</button>
      <button onclick="stopRecord()">Stop</button>
      <span id="recStatus" class="pill">idle</span>
    </div>
    <div class="meta">Recordings are stored under <code>/data/recordings/&lt;session&gt;/&lt;camera&gt;/raw|annotated/</code></div>
  </section>

  <section id="playback" class="section">
    <div class="controls">
      <button onclick="loadSessions()">Refresh Sessions</button>
      <select id="sessionSel" onchange="loadCameras()"></select>
      <select id="cameraSel" onchange="loadTimeline()"></select>
      <select id="kindSel" onchange="loadTimeline()">
        <option value="raw">raw</option>
        <option value="annotated">annotated</option>
      </select>
      <button onclick="play()">Play</button>
      <button onclick="pause()">Pause</button>
      <label>Rate <input id="rateSel" type="number" value="1.0" step="0.25" style="width:70px"/></label>
      <label>Filter class <input id="classFilter" type="text" placeholder="e.g. person"/></label>
      <button onclick="searchAndJump()">Search & Jump</button>
    </div>
    <div class="row">
      <div class="col">
        <input id="timeline" class="timeline" type="range" min="0" max="0" step="1" value="0" oninput="showFrame()"/>
        <div id="frameTime" class="meta">Time: â€”</div>
        <div class="imgwrap"><img id="playImg" /></div>
      </div>
      <div class="col">
        <div class="meta">If viewing RAW, you may open Annotated alongside in another tab; timeline indices align by timestamp.</div>
        <div id="hits" class="meta"></div>
      </div>
    </div>
  </section>

  <section id="stats" class="section">
    <div class="controls">
      <button onclick="loadSessions()">Refresh Sessions</button>
      <select id="statsSessionSel" onchange="loadStats()"></select>
    </div>
    <div id="chartWrap"><svg id="chart"></svg></div>
  </section>
  <script type="module">


    import * as THREE from "/static/js/three/build/three.webgpu.js";
    import * as WEBGPU from "/static/js/three/examples/jsm/capabilities/WebGPU.js";
    import * as Stats from "/static/js/three/examples/jsm/libs/stats.module.js";
    import { OrbitControls } from '/static/js/three/examples/jsm/controls/OrbitControls.js';

    import { Fn, wgslFn, positionLocal, scriptable, positionWorld, normalLocal, normalWorld, normalView, color, texture, uv, float, vec2, vec3, vec4, oscSine, triplanarTexture, screenUV, js, string, Loop, cameraProjectionMatrix, ScriptableNodeResources } from "/static/js/three/build/three.tsl.js";


    let lidarScene, lidarCamera, lidarRenderer, lidarPointsMesh, imuArrow, controls, lidarGeometry, lidarMaterial, renderer;
    let useWebGPU = false;


    async function initLidar() {
      const canvas = document.getElementById("lidarCanvas");
      useWebGPU = !!navigator.gpu;

      if (useWebGPU) {
        console.log("âœ… WEBGPU supported, using WEBGPU.WebGPURenderer");
        lidarRenderer = new THREE.WebGPURenderer({ canvas, antialias: true });
        lidarRenderer.setPixelRatio( window.devicePixelRatio );
        lidarRenderer.setSize( window.innerWidth, window.innerHeight );
        await lidarRenderer.init();

        // WebGPU node-based material
        lidarMaterial = new THREE.PointsNodeMaterial({
          size: 0.01,
          sizeAttenuation: true,
          vertexColors: true
        });
      } else {
        console.log("âš ï¸ WebGPU not available, using THREE.WebGLRenderer");
        lidarRenderer = new THREE.WebGLRenderer({ canvas, antialias: true });

        // WebGL shader material fallback
        lidarMaterial = new THREE.ShaderMaterial({
          vertexColors: true,
          transparent: true,
          depthTest: true,
          blending: THREE.AdditiveBlending,
          uniforms: { pointSize: { value: 0.01 } },
          vertexShader: `
            uniform float pointSize;
            varying vec3 vColor;
            void main() {
              vColor = color;
              vec4 mvPosition = modelViewMatrix * vec4(position, 1.0);
              gl_PointSize = pointSize * (300.0 / -mvPosition.z);
              gl_Position = projectionMatrix * mvPosition;
            }
          `,
          fragmentShader: `
            varying vec3 vColor;
            void main() {
              vec2 c = gl_PointCoord - vec2(0.5);
              if (dot(c, c) > 0.25) discard;
              gl_FragColor = vec4(vColor, 1.0);
            }
          `
        });
      }

      lidarCamera = new THREE.PerspectiveCamera(
        75,
        window.innerWidth / (window.innerHeight - 48),
        0.1,
        1000
      );
      lidarCamera.position.set(0, 0, 50);

      // ðŸ”¥ Create scene + camera before adding meshes
      lidarScene = new THREE.Scene();
      lidarScene.background = new THREE.Color(0x000000);

      // Initialize geometry with dummy attributes so WebGPU compiles clean
      lidarGeometry = new THREE.BufferGeometry();
      lidarGeometry.setAttribute('position', new THREE.BufferAttribute(new Float32Array([0, 0, 0]), 3));
      lidarGeometry.setAttribute('color', new THREE.BufferAttribute(new Float32Array([1, 1, 1]), 3));
      lidarGeometry.computeBoundingSphere();

      lidarPointsMesh = new THREE.Points(lidarGeometry, lidarMaterial);
      lidarScene.add(lidarPointsMesh);

      // controls, grid, axes, etc.
      controls = new OrbitControls(lidarCamera, lidarRenderer.domElement);
      controls.enableDamping = true;
      controls.dampingFactor = 0.05;

      const grid = new THREE.GridHelper(100, 100, 0x444444, 0x222222);
      lidarScene.add(grid);
      const axes = new THREE.AxesHelper(5);
      lidarScene.add(axes);

      const dir = new THREE.Vector3(1,0,0);
      imuArrow = new THREE.ArrowHelper(dir, new THREE.Vector3(0,0,0), 2, 0x00ff00);
      lidarScene.add(imuArrow);

      animateLidar();
    }

    function animateLidar() {
      requestAnimationFrame(animateLidar);
      controls.update();                           // âœ¨ update controls each frame
      lidarRenderer.render(lidarScene, lidarCamera);
    }

    function updateIMU(imu) {
      if (!imu || !imuArrow) return;
      const {roll, pitch, yaw} = imu;

      // Build quaternion from roll/pitch/yaw
      const euler = new THREE.Euler(roll, pitch, yaw, 'XYZ');
      const quat = new THREE.Quaternion().setFromEuler(euler);

      const dir = new THREE.Vector3(1,0,0).applyQuaternion(quat).normalize();
      imuArrow.setDirection(dir);
    }

    function intensityToColor(intensity) {
      const t0 = performance.now();
      // normalize 0..255 â†’ 0..1
      const t = Math.max(0, Math.min(1, intensity / 255.0));
      const t2 = performance.now();
      // simple "jet" style colormap: blue â†’ cyan â†’ yellow â†’ red
      const r = t < 0.5 ? 0 : (t - 0.5) * 2;
      const g = t < 0.5 ? t * 2 : (1 - (t - 0.5) * 2);
      const b = t < 0.5 ? 1 - t * 2 : 0;
      const t1 = performance.now();
      console.log(`intensityToColor: start ${(t1-t0).toFixed(1)} ms, finish ${(t2-t1).toFixed(1)} ms`);
      return [r, g, b];
      
    }

    function updateLidar(points) {
      if (!points || !points.length) return;

      const t0 = performance.now();

      const vertices = new Float32Array(points.length * 3);
      const colors   = new Float32Array(points.length * 3);

      for (let i = 0; i < points.length; i++) {
        let [x, y, z, intensity] = points[i];

        // Example: rotate 90Â° about X so ground plane lines up with grid
        // Original: z=up, y=forward â†’ want y=forward, z=up
        // Adjust as needed depending on your LiDARâ€™s convention
        const y2 = z;
        const z2 = -y;
        y = y2;
        z = z2;

        vertices[i*3+0] = x;
        vertices[i*3+1] = y;
        vertices[i*3+2] = z;

        const t = Math.max(0, Math.min(1, intensity / 255));
        colors[i*3+0] = t;
        colors[i*3+1] = 1.0 - t;
        colors[i*3+2] = 0.5;
      }


      const t1 = performance.now();

      lidarGeometry.setAttribute('position', new THREE.BufferAttribute(vertices, 3));
      lidarGeometry.setAttribute('color', new THREE.BufferAttribute(colors, 3));
      lidarGeometry.computeBoundingSphere();

      if (!useWebGPU) {
        // Only for WebGL ShaderMaterial
        lidarMaterial.uniforms.pointSize.value = 0.01;
        lidarMaterial.needsUpdate = true;
      }
      
      const t2 = performance.now();
      console.log(`updateLidar: build arrays ${(t1-t0).toFixed(1)} ms, upload ${(t2-t1).toFixed(1)} ms`);
    }

    
    window.updateLidar = updateLidar;
    window.updateIMU = updateIMU;
    initLidar();


  function makeJsonTile(id, label, objOrText) {                   
    const tile = document.createElement('div');
    tile.className = 'tile';
    tile.id = `tile-${label.toLowerCase()}-${id}`;
    const pretty = (typeof objOrText === 'string')
      ? objOrText
      : JSON.stringify(objOrText, null, 2);
    tile.innerHTML = `
      <header><h3>${id}</h3><span class="pill">${label}</span></header>
      <div class="meta" style="max-height:260px; overflow:auto;">
        <pre style="margin:0; white-space:pre-wrap;">${pretty ? escapeHtml(pretty) : 'â€”'}</pre>
      </div>
    `;
    return tile;
  }

  function makeImageTile(kind, id, b64, label) {                      // NEW
    const tile = document.createElement('div');
    tile.className = 'tile';
    tile.id = `tile-${kind}-${id}`;
    tile.innerHTML = `
      <header><h3>${id}</h3><span class="pill">${label}</span><span class="fs-hint">click image to fullscreen</span></header>
      <div class="imgwrap">
        ${b64 ? `<img id="img-${kind}-${id}" data-camid="${id}" src="data:image/jpeg;base64,${b64}" />`
               : `<div class="meta">Waiting for framesâ€¦</div>`}
      </div>
      <div class="orbit-wrap"><div class="glow-ball"></div></div>
    `;
    const img = tile.querySelector('img');
    if (img) img.onclick = () => toggleFullscreen(tile);
    return tile;
  }

  function escapeHtml(s) {                                            // NEW
    return (s || '').replace(/[&<>"']/g, c => ({'&':'&amp;','<':'&lt;','>':'&gt;','"':'&quot;',"'":'&#39;'}[c]));
  }

  function ensureJsonTile(gridEl, id, label, dataObj) {               // NEW
    const tid = `tile-${label.toLowerCase()}-${id}`;
    let tile = document.getElementById(tid);
    if (!tile) {
      tile = makeJsonTile(id, label, dataObj);
      gridEl.appendChild(tile);
    } else {
      const pre = tile.querySelector('pre');
      const pretty = (typeof dataObj === 'string') ? dataObj : JSON.stringify(dataObj, null, 2);
      if (pre) pre.textContent = pretty || 'â€”';
    }
    return tile;
  }

  function ensureImageTile(gridEl, kind, id, label, b64) {
    const tid = `tile-${kind}-${id}`;
    let tile = document.getElementById(tid);
    if (!tile) {
      tile = makeImageTile(kind, id, b64, label);
      gridEl.appendChild(tile);
    } else if (b64) {
      const img = tile.querySelector('img');
      const newSrc = `data:image/jpeg;base64,${b64}`;
      if (img && img.src !== newSrc) img.src = newSrc;
    }
    return tile;
  }


  function toggleFullscreen(tile) {
    const el = tile;
    if (!document.fullscreenElement) {
      if (el.requestFullscreen) el.requestFullscreen();
      else if (el.webkitRequestFullscreen) el.webkitRequestFullscreen();
    } else {
      if (document.exitFullscreen) document.exitFullscreen();
      else if (document.webkitExitFullscreen) document.webkitExitFullscreen();
    }
  }
  function makeTile(id, label, b64) {
    const kind = (label || '').toLowerCase(); // "raw" or "annotated"
    const tile = document.createElement('div');
    tile.className = 'tile';
    tile.id = `tile-${kind}-${id}`;
    tile.innerHTML = `
      <header><h3>${id}</h3><span class="pill">${label}</span><span class="fs-hint">click image to fullscreen</span></header>
      <div class="imgwrap">
        ${b64 ? `<img id="img-${kind}-${id}" data-camid="${id}" src="data:image/jpeg;base64,${b64}" />`
              : `<div class="meta">Waiting for framesâ€¦</div>`}
      </div>
      <div class="orbit-wrap">
        <div class="glow-ball"></div>
      </div>
    `;
    const img = tile.querySelector('img');
    if (img) img.onclick = () => toggleFullscreen(tile);
    return tile;
  }


    // Tabs
    document.querySelectorAll('.tab').forEach(t => {
      t.addEventListener('click', () => {
        document.querySelectorAll('.tab').forEach(x => x.classList.remove('active'));
        document.querySelectorAll('.section').forEach(x => x.classList.remove('active'));
        t.classList.add('active');
        document.getElementById(t.dataset.target).classList.add('active');
      });
    });

    // Live WS for Raw/Annotated/Detections/Alerts
    const ws = new WebSocket("wss://" + location.host + "/ws");

    function ensureTile(gridEl, id, label, initialB64=null) {
      const kind = label.toLowerCase();
      const tileId = `tile-${kind}-${id}`;
      let tile = document.getElementById(tileId);
      if (!tile) {
        tile = makeTile(id, label, initialB64); // use initial frame if available
        gridEl.appendChild(tile);
      }
      return tile;
    }


    function setMotion(tile, inMotion) {
      const ball = tile.querySelector('.glow-ball');
      if (inMotion) {
        tile.classList.add('motion');
        if (ball) ball.style.display = 'block';
      } else {
        tile.classList.remove('motion');
        if (ball) ball.style.display = 'none';
      }
    }

    function updateImage(kind, camId, b64) {
      if (!b64) return;

      const newSrc = `data:image/jpeg;base64,${b64}`;
      let img = document.getElementById(`img-${kind}-${camId}`);

      if (!img) {
        // no <img> yet -> replace the placeholder in this tile
        const tile = document.getElementById(`tile-${kind}-${camId}`);
        if (!tile) return;
        const wrap = tile.querySelector('.imgwrap');
        wrap.innerHTML = `<img id="img-${kind}-${camId}" data-camid="${camId}" />`;
        img = document.getElementById(`img-${kind}-${camId}`);
        img.onclick = () => toggleFullscreen(tile);
      }

      if (img.src !== newSrc) img.src = newSrc;
    }

    ws.onmessage = (event) => {
      const T = () => performance.now();
      const t0 = T();
      const payloadMB = (event.data.length / 1e6).toFixed(2);

      // --- parse ---
      const p0 = T();
      const data = JSON.parse(event.data);
      const p1 = T();
      const parseMs = p1 - p0;

      // Build a stable list of cameras
      const cams = new Set(Object.keys(data.known_cameras || {}));
      Object.keys(data.raw || {}).forEach(id => cams.add(id));
      Object.keys(data.annotated || {}).forEach(id => cams.add(id));
      Object.keys(data.embeddings || {}).forEach(id => cams.add(id));
      Object.keys(data.df_analysis || {}).forEach(id => cams.add(id));
      Object.keys(data.df_annotated || {}).forEach(id => cams.add(id));
      const camList = Array.from(cams).sort();

      const rawGrid = document.getElementById('rawGrid');
      const annGrid = document.getElementById('annGrid');
      const embGrid = document.getElementById('embGrid');         // NEW
      const dfaGrid = document.getElementById('dfaGrid');         // NEW
      const dfaAnnGrid = document.getElementById('dfaAnnGrid');   // NEW

      // If something is fullscreen, don't rebuild grids; only update that tile's img & motion
      const fs = document.fullscreenElement || document.webkitFullscreenElement;

      const t1 = T();
      console.log(`onmessage: data load ${(t1-t0).toFixed(1)} ms`);

      let rawMs=0, annMs=0, embMs=0, dfaMs=0, dfannMs=0, detMs=0, alertMs=0, lidarMs=0;
      let ensureTileMs=0, updateImageMs=0, ensureJsonTileMs=0, ensureImageTileMs=0, imuMs=0, overlayMs=0;

      if (!fs) {
        // ---- RAW
        const t2 = T();
        camList.forEach(id => {
          const b64 = (data.raw && data.raw[id]) ? data.raw[id] : null;
          const ei0=T(); const tile = ensureTile(rawGrid, id, 'RAW', b64); const ei1=T(); ensureTileMs+=ei1-ei0;
          const inMotion = !!(data.motion && data.motion[id]);
          setMotion(tile, inMotion);
          const ui0=T(); updateImage('raw', id, b64); const ui1=T(); updateImageMs+=ui1-ui0;
        });
        const t3 = T();
        rawMs = t3 - t2;
        console.log(`onmessage: raw ${rawMs.toFixed(1)} ms`);

        // ---- ANNOTATED
        camList.forEach(id => {
          const b64 = (data.annotated && data.annotated[id]) ? data.annotated[id] : null;
          const ei0=T(); const tile = ensureTile(annGrid, id, 'ANNOTATED', b64); const ei1=T(); ensureTileMs+=ei1-ei0;
          const inMotion = !!(data.motion && data.motion[id]);
          setMotion(tile, inMotion);
          const ui0=T(); updateImage('annotated', id, b64); const ui1=T(); updateImageMs+=ui1-ui0;
        });
        const t4 = T();
        annMs = t4 - t3;
        console.log(`onmessage: annotated ${annMs.toFixed(1)} ms`);

        // ---- EMBEDDINGS
        if (embGrid) {
          camList.forEach(id => {
            const obj = data.embeddings ? data.embeddings[id] : null;
            if (obj !== undefined) {
              const ej0=T(); ensureJsonTile(embGrid, id, 'EMBEDDINGS', obj); const ej1=T(); ensureJsonTileMs+=ej1-ej0;
            }
          });
        }
        const t5 = T();
        embMs = t5 - t4;
        console.log(`onmessage: embeddings ${embMs.toFixed(1)} ms`);

        // ---- DF ANALYSIS
        if (dfaGrid) {
          camList.forEach(id => {
            const obj = data.df_analysis ? data.df_analysis[id] : null;
            if (obj !== undefined) {
              const ej0=T(); ensureJsonTile(dfaGrid, id, 'DF_ANALYSIS', obj); const ej1=T(); ensureJsonTileMs+=ej1-ej0;
            }
          });
        }
        const t6 = T();
        dfaMs = t6 - t5;
        console.log(`onmessage: dfaGrid ${dfaMs.toFixed(1)} ms`);

        // ---- DF ANNOTATED
        if (dfaAnnGrid) {
          camList.forEach(id => {
            const b64 = (data.df_annotated && data.df_annotated[id]) ? data.df_annotated[id] : null;
            if (b64 !== undefined) {
              const ei0=T(); const tile = ensureImageTile(dfaAnnGrid, 'dfann', id, 'DF_ANNOTATED', b64); const ei1=T(); ensureImageTileMs+=ei1-ei0;
              const inMotion = !!(data.motion && data.motion[id]);
              setMotion(tile, inMotion);
              const ui0=T(); updateImage('dfann', id, b64); const ui1=T(); updateImageMs+=ui1-ui0;
            }
          });
        }
        const t7 = T();
        dfannMs = t7 - t6;
        console.log(`onmessage: dfaAnnGrid ${dfannMs.toFixed(1)} ms`);

      } else {
        const t8 = T();
        // Only patch the fullscreen tile
        const fsEl = fs;
        const isRaw = fsEl.id && fsEl.id.startsWith('tile-raw-');
        const isAnn = fsEl.id && fsEl.id.startsWith('tile-annotated-');
        if (isRaw || isAnn) {
          const kind = isRaw ? 'raw' : 'annotated';
          const camId = fsEl.id.replace(`tile-${kind}-`, '');
          const b64 = (data[kind] && data[kind][camId]) ? data[kind][camId] : null;
          const ui0=T(); updateImage(kind, camId, b64); const ui1=T(); updateImageMs+=ui1-ui0;
          const inMotion = !!(data.motion && data.motion[camId]);
          setMotion(fsEl, inMotion);
        }
        const t9 = T();
        console.log(`onmessage: isRaw || isAnn ${(t9-t8).toFixed(1)} ms`);
      }

      console.log("Lidar frame:", data.lidar?.length);

      if (data.lidar) {
        const l0 = T();
        updateLidar(data.lidar);
        const l1 = T();
        lidarMs = l1 - l0;
        console.log(`onmessage: lidar ${lidarMs.toFixed(1)} ms`);
      }

      if (data.lidar_imu) {
        const im0=T(); updateIMU(data.lidar_imu); const im1=T(); imuMs+=im1-im0;
      }

      if (typeof data.lidar_loss !== "undefined") {
        const ov0=T(); document.getElementById("lidarOverlay").textContent = "Loss: " + data.lidar_loss; const ov1=T(); overlayMs+=ov1-ov0;
      }

      // ---- Detections
      const detWrap = document.getElementById('detWrap');
      detWrap.innerHTML = '';
      const d0 = T();
      if (data.detections) {
        camList.forEach(id => {
          const dets = data.detections[id] || [];
          const div = document.createElement('div');
          div.className = 'tile';
          div.innerHTML = `
            <header><h3>${id}</h3><span class="pill">DETECTIONS</span></header>
            <div class="meta">${dets.length ? dets.map(d => `${d.class} (${(d.score*100).toFixed(1)}%) [${d.bbox.map(x=>x.toFixed(1)).join(", ")}]`).join("<br>") : "No detections"}</div>
          `;
          detWrap.appendChild(div);
        });
      }
      const d1 = T();
      detMs = d1 - d0;
      console.log(`onmessage: detections ${detMs.toFixed(1)} ms`);

      // ---- Alerts
      const alertWrap = document.getElementById('alertWrap');
      alertWrap.innerHTML = '';
      const alerts = data.alerts || [];
      const a0 = T();
      if (!alerts.length) {
        alertWrap.innerHTML = '<div class="meta">No alerts</div>';
      } else {
        alerts.slice(-100).reverse().forEach(a => {
          const div = document.createElement('div');
          div.className = 'alertbox';
          const when = a.time ? new Date(a.time*1000).toLocaleString() : "â€”";
          div.innerHTML = `<b>${a.class}</b> on <b>${a.camera}</b> (${Math.round((a.score||0)*100)}%) at ${when}`;
          alertWrap.appendChild(div);
        });
      }
      const a1 = T();
      alertMs = a1 - a0;
      console.log(`onmessage: alertWrap ${alertMs.toFixed(1)} ms`);

      const t2 = T();
      const processingMs = t2 - t1;
      const accounted = rawMs + annMs + embMs + dfaMs + dfannMs + lidarMs + detMs + alertMs +
                        ensureTileMs + updateImageMs + ensureJsonTileMs + ensureImageTileMs + imuMs + overlayMs;
      const unaccounted = processingMs - accounted;

      console.log(
        `onmessage: payload ${payloadMB} MB | parse ${parseMs.toFixed(1)} ms | ` +
        `processing ${processingMs.toFixed(1)} ms ` +
        `(raw ${rawMs.toFixed(1)}, ann ${annMs.toFixed(1)}, emb ${embMs.toFixed(1)}, ` +
        `dfa ${dfaMs.toFixed(1)}, dfann ${dfannMs.toFixed(1)}, lidar ${lidarMs.toFixed(1)}, ` +
        `det ${detMs.toFixed(1)}, alert ${alertMs.toFixed(1)}, ` +
        `ensureTile ${ensureTileMs.toFixed(1)}, updateImage ${updateImageMs.toFixed(1)}, ` +
        `ensureJsonTile ${ensureJsonTileMs.toFixed(1)}, ensureImageTile ${ensureImageTileMs.toFixed(1)}, ` +
        `imu ${imuMs.toFixed(1)}, overlay ${overlayMs.toFixed(1)}) | ` +
        `unaccounted ${unaccounted.toFixed(1)} ms`
      );
    };

    // ---- Record tab ----
    async function startRecord() {
      const name = document.getElementById('sessionName').value.trim();
      const res = await fetch('/api/record/start', {method:'POST', headers:{'Content-Type':'application/json'}, body: JSON.stringify({session:name||null})});
      const data = await res.json();
      document.getElementById('recStatus').textContent = 'recording: '+data.session;
      loadSessions();
      // also set stats selector
      const sSel = document.getElementById('statsSessionSel');
      if (sSel) loadSessions();
    }
    async function stopRecord() {
      const res = await fetch('/api/record/stop', {method:'POST'});
      const data = await res.json();
      document.getElementById('recStatus').textContent = 'stopped';
      loadSessions();
    }

    // ---- Playback tab ----
    async function loadSessions() {
      const res = await fetch('/api/sessions');
      const data = await res.json();
      const sel = document.getElementById('sessionSel');
      const sSel = document.getElementById('statsSessionSel');
      sel.innerHTML = '';
      sSel.innerHTML = '';
      data.sessions.forEach(s => {
        const o = document.createElement('option'); o.value=s; o.textContent=s; sel.appendChild(o);
        const o2 = document.createElement('option'); o2.value=s; o2.textContent=s; sSel.appendChild(o2);
      });
      if (data.sessions.length) { sel.value = data.sessions[data.sessions.length-1]; sSel.value = sel.value; }
      await loadCameras(); await loadStats();
    }

    async function loadCameras() {
      const ses = document.getElementById('sessionSel').value;
      const res = await fetch('/api/session/'+encodeURIComponent(ses)+'/cameras');
      const data = await res.json();
      const csel = document.getElementById('cameraSel');
      csel.innerHTML = '';
      data.cameras.forEach(c => { const o = document.createElement('option'); o.value=c; o.textContent=c; csel.appendChild(o); });
      await loadTimeline();
    }

    let frameIndex = 0;
    let frameList = [];
    async function loadTimeline() {
      const ses = document.getElementById('sessionSel').value;
      const cam = document.getElementById('cameraSel').value;
      const kind = document.getElementById('kindSel').value;
      const res = await fetch(`/api/frames?session=${encodeURIComponent(ses)}&camera=${encodeURIComponent(cam)}&kind=${encodeURIComponent(kind)}`);
      const data = await res.json();
      frameList = data.frames || [];
      const slider = document.getElementById('timeline');
      slider.min = 0; slider.max = Math.max(0, frameList.length-1); slider.value = 0;
      frameIndex = 0;
      showFrame();
      document.getElementById('hits').innerHTML = '';
    }

    async function showFrame() {
      const slider = document.getElementById('timeline');
      frameIndex = parseInt(slider.value || '0');
      const meta = frameList[frameIndex];
      const img = document.getElementById('playImg');
      const tdiv = document.getElementById('frameTime');
      if (!meta) { img.src=''; tdiv.textContent='Time: â€”'; return; }
      img.src = '/media?path='+encodeURIComponent(meta.path);
      tdiv.textContent = 'Time: ' + new Date(meta.ts*1000).toLocaleString();
    }

    let playTimer = null;
    function play() {
      if (playTimer) return;
      const rate = parseFloat(document.getElementById('rateSel').value || '1');
      playTimer = setInterval(() => {
        if (frameIndex < frameList.length-1) {
          frameIndex++;
          document.getElementById('timeline').value = frameIndex;
          showFrame();
        } else {
          pause();
        }
      }, 1000 / Math.max(0.1, rate));
    }
    function pause() { if (playTimer) { clearInterval(playTimer); playTimer=null; } }

    async function searchAndJump() {
      const ses = document.getElementById('sessionSel').value;
      const cam = document.getElementById('cameraSel').value;
      const klass = document.getElementById('classFilter').value.trim();
      if (!klass) { document.getElementById('hits').textContent = 'Enter a class to search.'; return; }
      const res = await fetch(`/api/search?session=${encodeURIComponent(ses)}&camera=${encodeURIComponent(cam)}&class=${encodeURIComponent(klass)}`);
      const data = await res.json();
      const hits = data.hits || [];
      document.getElementById('hits').innerHTML = hits.length ? hits.map(h => {
        return `<a href="#" onclick="jumpTo(${h.ts})">${new Date(h.ts*1000).toLocaleString()}</a>`;
      }).join('<br>') : 'No matches.';
    }
    function jumpTo(ts) {
      // find nearest frame index
      let best = 0, bestDiff = 1e18;
      for (let i=0;i<frameList.length;i++) {
        const d = Math.abs(frameList[i].ts - ts);
        if (d < bestDiff) { bestDiff=d; best=i; }
      }
      frameIndex = best;
      const slider = document.getElementById('timeline');
      slider.value = best;
      showFrame();
    }

    // ---- Stats tab (detections per camera per class) ----
    async function loadStats() {
      const ses = document.getElementById('statsSessionSel').value;
      if (!ses) return;
      const res = await fetch(`/api/stats/detections?session=${encodeURIComponent(ses)}`);
      const data = await res.json();
      drawChart(data);
    }

    function drawChart(data) {
      // data: { camera1: {classA: count, classB: count}, camera2: {...} }
      const svg = document.getElementById('chart');
      while (svg.firstChild) svg.removeChild(svg.firstChild);

      const cams = Object.keys(data).sort();
      const classes = Array.from(new Set(cams.flatMap(c => Object.keys(data[c])))).sort();

      const pad = 40, W = svg.clientWidth||800, H = svg.clientHeight||320;
      const innerW = W - pad*2, innerH = H - pad*2;

      const camCount = Math.max(1, cams.length);
      const classCount = Math.max(1, classes.length);
      const groupW = innerW / camCount;
      const barW = Math.max(6, groupW / (classCount+1));
      const maxVal = Math.max(1, Math.max(...cams.map(c => Math.max(1, ...Object.values(data[c]||{_0:0})))));
      const scaleY = v => pad + innerH - (v / maxVal) * innerH;

      // axes
      const axis = (x1,y1,x2,y2) => {
        const l = document.createElementNS("http://www.w3.org/2000/svg", "line");
        l.setAttribute("x1", x1); l.setAttribute("y1", y1);
        l.setAttribute("x2", x2); l.setAttribute("y2", y2);
        l.setAttribute("stroke", "#3b4254");
        svg.appendChild(l);
      };
      axis(pad, pad, pad, pad+innerH);
      axis(pad, pad+innerH, pad+innerW, pad+innerH);

      // bars
      cams.forEach((cam, ci) => {
        classes.forEach((cls, ki) => {
          const v = (data[cam] && data[cam][cls]) ? data[cam][cls] : 0;
          const x = pad + ci*groupW + ki*barW + 4;
          const y = scaleY(v);
          const h = pad + innerH - y;
          const r = document.createElementNS("http://www.w3.org/2000/svg", "rect");
          r.setAttribute("x", x); r.setAttribute("y", y);
          r.setAttribute("width", barW-6); r.setAttribute("height", h);
          r.setAttribute("fill", "#4b5563");
          svg.appendChild(r);
        });
        // cam label
        const tx = document.createElementNS("http://www.w3.org/2000/svg", "text");
        tx.setAttribute("x", pad + ci*groupW + groupW/2);
        tx.setAttribute("y", pad + innerH + 16);
        tx.setAttribute("text-anchor", "middle");
        tx.setAttribute("fill", "#a1a7b5");
        tx.setAttribute("font-size", "11");
        tx.textContent = cam;
        svg.appendChild(tx);
      });

      // simple legend
      classes.forEach((cls, i) => {
        const lx = pad + i*100, ly = 16;
        const rect = document.createElementNS("http://www.w3.org/2000/svg", "rect");
        rect.setAttribute("x", lx); rect.setAttribute("y", ly);
        rect.setAttribute("width", 14); rect.setAttribute("height", 10);
        rect.setAttribute("fill", "#4b5563");
        svg.appendChild(rect);
        const lt = document.createElementNS("http://www.w3.org/2000/svg", "text");
        lt.setAttribute("x", lx+20); lt.setAttribute("y", ly+10);
        lt.setAttribute("fill", "#cbd5e1");
        lt.setAttribute("font-size", "12");
        lt.textContent = cls;
        svg.appendChild(lt);
      });
    }
    // init
    loadSessions();
    
  </script>

</body>
</html>
""")

# ---------------- WebSocket: periodic live push ----------------
@app.websocket("/ws")
async def ws_endpoint(ws: WebSocket):
    await ws.accept()
    try:
        while True:
            now = time.time()
            motion = {cid: (now < _motion_until.get(cid, 0)) for cid in known_cameras}

            payload = {
                "known_cameras": {cid: True for cid in known_cameras},
                "raw": latest_raw_frames,
                "annotated": latest_ann_frames,
                "detections": latest_detections,
                "alerts": latest_alerts,
                "motion": motion,
                "embeddings": latest_embeddings,
                "df_analysis": latest_df_analysis,
                "df_annotated": latest_df_ann_frames,
                "lidar": latest_lidar_points,
                "lidar_imu": latest_lidar_imu,
                "lidar_loss": latest_lidar_loss,
            }
            await ws.send_text(json.dumps(payload))
            await asyncio.sleep(0.25)
    except Exception:
        pass

# ---------------- Recording REST API ----------------
@app.post("/api/record/start")
async def api_record_start(body: Dict[str, Any]):
    global recording_enabled, recording_session
    name = body.get("session") if body else None
    if not name or not str(name).strip():
        # default session name
        name = datetime.now().strftime("rec_%Y%m%d_%H%M%S")
    with recording_lock:
        recording_session = name
        recording_enabled = True
    return JSONResponse({"ok": True, "session": name})

@app.post("/api/record/stop")
async def api_record_stop():
    global recording_enabled
    with recording_lock:
        recording_enabled = False
    return JSONResponse({"ok": True})

# ---------------- Playback / Search REST API ----------------
@app.get("/api/sessions")
async def api_sessions():
    # sessions inferred from frames table
    rows = db_query("SELECT DISTINCT session FROM frames ORDER BY session;")
    sessions = [r[0] for r in rows]
    return JSONResponse({"sessions": sessions})

@app.get("/api/session/{session}/cameras")
async def api_session_cameras(session: str):
    rows = db_query("SELECT DISTINCT camera FROM frames WHERE session=? ORDER BY camera;", (session,))
    cams = [r[0] for r in rows]
    return JSONResponse({"cameras": cams})

@app.get("/api/frames")
async def api_frames(session: str, camera: str, kind: str, t0: float = Query(None), t1: float = Query(None)):
    if t0 is not None and t1 is not None:
        rows = db_query(
            "SELECT ts, path FROM frames WHERE session=? AND camera=? AND kind=? AND ts BETWEEN ? AND ? ORDER BY ts;",
            (session, camera, kind, t0, t1)
        )
    else:
        rows = db_query(
            "SELECT ts, path FROM frames WHERE session=? AND camera=? AND kind=? ORDER BY ts;",
            (session, camera, kind)
        )
    frames = [{"ts": r[0], "path": r[1]} for r in rows]
    return JSONResponse({"frames": frames})

@app.get("/media")
async def api_media(path: str):
    # Basic security: ensure within RECORD_ROOT
    full = osp.realpath(path)
    root = osp.realpath(RECORD_ROOT)
    if not full.startswith(root):
        return JSONResponse({"error": "invalid path"}, status_code=400)
    if not osp.exists(full):
        return JSONResponse({"error": "not found"}, status_code=404)
    return FileResponse(full, media_type="image/jpeg")

@app.get("/api/search")
async def api_search(
    session: str,
    camera: str,
    class_: str = Query(default=None, alias="class")
):
    clazz = class_ if class_ is not None else "class"
    rows = db_query(
        "SELECT ts FROM detections WHERE session=? AND camera=? AND class=? ORDER BY ts;",
        (session, camera, clazz)
    )
    hits = [{"ts": r[0]} for r in rows]
    return JSONResponse({"hits": hits})

@app.get("/api/stats/detections")
async def api_stats_detections(session: str):
    rows = db_query(
        "SELECT camera, class, COUNT(1) FROM detections WHERE session=? GROUP BY camera, class;",
        (session,)
    )
    out: Dict[str, Dict[str, int]] = {}
    for cam, cls, cnt in rows:
        out.setdefault(cam, {})[cls] = int(cnt)
    return JSONResponse(out)

# ---------------- Spin ROS + HTTP ----------------
def ros_thread():
    rclpy.init()
    node = WebStreamNode()
    try:
        rclpy.spin(node)
    finally:
        node.destroy_node()
        rclpy.shutdown()

def main():
    # Resolve packaged SSL files
    ssl_root = files('wildlife_webviewer') / 'ssl'
    cert_path = str(ssl_root / 'cert.pem')
    key_path  = str(ssl_root / 'key.pem')

    #  allow override via env vars
    cert_path = osp.abspath(os.environ.get('WVW_CERT', cert_path))
    key_path  = osp.abspath(os.environ.get('WVW_KEY',  key_path))

    t = threading.Thread(target=ros_thread, daemon=True)
    t.start()
    uvicorn.run(
        app,
        host="0.0.0.0",
        port=1080,
        ssl_certfile=cert_path,
        ssl_keyfile=key_path,
    )

if __name__ == "__main__":
    main()
